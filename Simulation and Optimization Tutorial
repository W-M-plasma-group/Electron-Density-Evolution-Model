%matplotlib inline
import time
import Model_Classes as CLSS

################ Simulation Tutorial ################

#step 1.) Select the data you want to model and compare to
  # i.) the experiemental data from before (density, position, time)
  # ii.) the model parameters you want to simulate with (source amplitude, Source decay rate, D1, D2, V1, V2)
  
real_comp_data_files = ["175888_density", "175888_rho", "175888_time"] # experimental data to call from a CVS file
synth_comp_data_params = [0,0,0,0,0,0] # example transport parameters for the siulation to use



#step 2.) call the model function from the Density_Model
DM_synth = CLSS.Density_Model(real_comp_data_files)  # this initiallizes the class
DM_synth.format_data() # this sets the experimental data to be formatted for the simulation
synth_comp_data = DM_synth.model(synth_comp_data_params) # this is the actual simulation setp. The output of this function is [sim density, rho, time]



# this section plots the simulated data
r,t = np.meshgrid(modeled_data[1],modeled_data[2])
plt.pcolor(r,t,synth_comp_data[0],shading = 'auto')
plt.colorbar()


################ Optimization Tutorial ################
#step 1.) set a starting point for the optimizer to begin, and a max number of iterations for it to terminate
init_guess = [0,0,0,0,0,0]
iterations = 1500

#step 2.) select the experimental data to be optimized and format it using the simulation class
real_comp_data_files = ["158348(2)_density", "158348(2)_rho", "158348(2)_time"]

DM_synth = CLSS.Density_Model(real_comp_data_files)
DM_synth.format_data()

# step 3.) initialize the optimization class with the formatted data from the simulation class
Opt_synth = CLSS.Optimization(DM_synth.original_density_data , real_comp_data_files) 

#step 4.) call the optimization function
start_time = time.time()
opt_vals = Opt_synth.optimize(init_guess,iterations).x #this function returns lots of info, but this just returns the actual optimized values
end_time = time.time()
    
# step 5.) get some performance info and results
total_time =(end_time - start_time)/ 60
    
optimized_info = [opt_vals, Opt_synth.chi2_difference(opt_vals), Opt_synth.chi2_difference(opt_vals, True), total_time]

print("Found Transport Values = ", optimized_info[0])
print("Difference = ", optimized_info[1])
print("Normalized Difference = ", optimized_info[2])
print("Time to Optimize = ", optimized_info[3])

print('total time to run:', (time.time() - start_time)/60)
